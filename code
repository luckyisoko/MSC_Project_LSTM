#importing relevant libraries
import yfinance as yf
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.ensemble import IsolationForest
from sklearn.cluster import KMeans
from sklearn.model_selection import KFold, ParameterGrid
from sklearn.exceptions import NotFittedError
import seaborn as sns

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# List the companies and date range
company_name = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'HSBA.L', 'LLOY.L']
start_date = '2019-01-01'
end_date = '2024-07-31'
prediction_start_date = '2024-05-01'
prediction_end_date = '2024-07-31'

# Download data from Yahoo Finance
data = yf.download(company_name, start=start_date, end=end_date)['Adj Close']
data.reset_index(inplace=True)
data['Date'] = pd.to_datetime(data['Date'])

# Forward fill NaN values
data.fillna(method='ffill', inplace=True)

# Plot the closing prices of the selected companies
plt.figure(figsize=(14, 7))
for symbol in company_name:
    plt.plot(data['Date'], data[symbol], label=symbol)
plt.title('Closing Prices of Selected Companies')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.show()

# Detecting outliers using IsolationForest
iso = IsolationForest(contamination=0.01)
outliers = iso.fit_predict(data[company_name])

# Remove outliers
mask = outliers != -1
data = data[mask].reset_index(drop=True)

# Normalize data using MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = pd.DataFrame(scaler.fit_transform(data[company_name]), columns=company_name)
scaled_data['Date'] = data['Date'].values

# Compute and visualize the correlation matrix
correlation_matrix = scaled_data[company_name].corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix of Selected Companies')
plt.show()

# Perform KMeans clustering based on the correlation matrix
kmeans = KMeans(n_clusters=2, random_state=0)
clusters = kmeans.fit_predict(correlation_matrix)

# Split data into two groups based on cluster labels
group1_symbols = [company_name[i] for i in range(len(company_name)) if clusters[i] == 0]
group2_symbols = [company_name[i] for i in range(len(company_name)) if clusters[i] == 1]

group1 = scaled_data[['Date'] + group1_symbols]
group2 = scaled_data[['Date'] + group2_symbols]

# Display the split results
print("Group 1 Symbols:", group1_symbols)
print(group1.head())
print("\nGroup 2 Symbols:", group2_symbols)
print(group2.head())

# Features and Label preparation
def create_sequences(data, seq_length):
    xs, ys = [], []
    for i in range(len(data) - seq_length):
        x = data[i:i+seq_length]
        y = data[i+seq_length]
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

# Convert data to sequences
seq_length = 60
group1_X, group1_Y = create_sequences(group1.drop('Date', axis=1).values, seq_length)
group2_X, group2_Y = create_sequences(group2.drop('Date', axis=1).values, seq_length)

# Split into train, validation, and test sets
def split_data(X, Y):
    train_size = int(len(X) * 0.7)
    val_size = int(len(X) * 0.2)
    test_size = len(X) - train_size - val_size

    X_train, X_val, X_test = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:]
    Y_train, Y_val, Y_test = Y[:train_size], Y[train_size:train_size+val_size], Y[train_size+val_size:]
    
    return X_train, X_val, X_test, Y_train, Y_val, Y_test

group1_X_train, group1_X_val, group1_X_test, group1_Y_train, group1_Y_val, group1_Y_test = split_data(group1_X, group1_Y)
group2_X_train, group2_X_val, group2_X_test, group2_Y_train, group2_Y_val, group2_Y_test = split_data(group2_X, group2_Y)

# Fit the scaler on the training data for group 1 and group 2 separately
scaler_group1 = MinMaxScaler(feature_range=(0, 1))
scaler_group1.fit(group1_X_train.reshape(-1, group1_X_train.shape[-1]))

scaler_group2 = MinMaxScaler(feature_range=(0, 1))
scaler_group2.fit(group2_X_train.reshape(-1, group2_X_train.shape[-1]))

# Apply scaling to train, validation, and test sets
group1_X_train_scaled = scaler_group1.transform(group1_X_train.reshape(-1, group1_X_train.shape[-1])).reshape(group1_X_train.shape)
group1_X_val_scaled = scaler_group1.transform(group1_X_val.reshape(-1, group1_X_val.shape[-1])).reshape(group1_X_val.shape)
group1_X_test_scaled = scaler_group1.transform(group1_X_test.reshape(-1, group1_X_test.shape[-1])).reshape(group1_X_test.shape)

group2_X_train_scaled = scaler_group2.transform(group2_X_train.reshape(-1, group2_X_train.shape[-1])).reshape(group2_X_train.shape)
group2_X_val_scaled = scaler_group2.transform(group2_X_val.reshape(-1, group2_X_val.shape[-1])).reshape(group2_X_val.shape)
group2_X_test_scaled = scaler_group2.transform(group2_X_test.reshape(-1, group2_X_test.shape[-1])).reshape(group2_X_test.shape)

# Define the LSTM Model
class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):
        super(LSTMModel, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.relu = nn.ReLU()

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)
        
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(self.relu(out[:, -1, :]))
        return out

# Function to perform cross-validation
def cross_val_score_model(model, X_train, Y_train, criterion, optimizer, num_epochs=10, n_splits=5):
    kf = KFold(n_splits=n_splits)
    val_losses = []
    
    for train_idx, val_idx in kf.split(X_train):
        model.train()
        
        X_train_fold = torch.tensor(X_train[train_idx]).float().to(device)
        Y_train_fold = torch.tensor(Y_train[train_idx]).float().to(device)
        X_val_fold = torch.tensor(X_train[val_idx]).float().to(device)
        Y_val_fold = torch.tensor(Y_train[val_idx]).float().to(device)
        
        for epoch in range(num_epochs):
            optimizer.zero_grad()
            outputs = model(X_train_fold)
            loss = criterion(outputs, Y_train_fold)
            loss.backward()
            optimizer.step()
        
        model.eval()
        val_outputs = model(X_val_fold)
        val_loss = criterion(val_outputs, Y_val_fold)
        val_losses.append(val_loss.item())
    
    return np.mean(val_losses)

# Hyperparameter grid
param_grid = {
    'hidden_dim': [50, 100, 150],
    'num_layers': [1, 2, 3],
    'lr': [0.01, 0.001, 0.0001]
}

# Function to fine-tune hyperparameters for a group
def fine_tune_hyperparameters(X_train, Y_train, input_dim, output_dim):
    results = []
    for params in ParameterGrid(param_grid):
        model = LSTMModel(input_dim=input_dim, hidden_dim=params['hidden_dim'], num_layers=params['num_layers'], output_dim=output_dim).to(device)
        criterion = nn.MSELoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])
        
        val_loss = cross_val_score_model(model, X_train, Y_train, criterion, optimizer)
        results.append((params, val_loss))
    
    best_params, best_loss = sorted(results, key=lambda x: x[1])[0]
    return best_params, best_loss

# Fine-tune hyperparameters for group 1 and group 2
group1_best_params, group1_best_loss = fine_tune_hyperparameters(group1_X_train_scaled, group1_Y_train, len(group1_symbols), len(group1_symbols))
group2_best_params, group2_best_loss = fine_tune_hyperparameters(group2_X_train_scaled, group2_Y_train, len(group2_symbols), len(group2_symbols))

print("Group 1 Best Params:", group1_best_params)
print("Group 2 Best Params:", group2_best_params)

# Function to check if scaler is fitted
def is_fitted(scaler):
    try:
        # Dynamically create a test input with the same number of features as the scaler was fitted on
        test_input = np.zeros((1, scaler.n_features_in_))
        scaler.transform(test_input)
        return True
    except NotFittedError:
        return False

# Check if scalers are fitted
print("Is Group 1 Scaler fitted?", is_fitted(scaler_group1))
print("Is Group 2 Scaler fitted?", is_fitted(scaler_group2))

# Training the model on the best parameters for Group 1 and Group 2
def train_model(X_train, Y_train, X_val, Y_val, best_params, input_dim, output_dim, num_epochs=100):
    model = LSTMModel(input_dim=input_dim, hidden_dim=best_params['hidden_dim'], num_layers=best_params['num_layers'], output_dim=output_dim).to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'])

    train_losses, val_losses = [], []

    for epoch in range(num_epochs):
        model.train()
        optimizer.zero_grad()
        outputs = model(torch.tensor(X_train).float().to(device))
        loss = criterion(outputs, torch.tensor(Y_train).float().to(device))
        loss.backward()
        optimizer.step()
        train_losses.append(loss.item())

        model.eval()
        val_outputs = model(torch.tensor(X_val).float().to(device))
        val_loss = criterion(val_outputs, torch.tensor(Y_val).float().to(device))
        val_losses.append(val_loss.item())

        if epoch % 10 == 0:
            print(f'Epoch [{epoch}/{num_epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')

    return model, train_losses, val_losses

# Train the model for Group 1
group1_model, group1_train_losses, group1_val_losses = train_model(group1_X_train_scaled, group1_Y_train, group1_X_val_scaled, group1_Y_val, group1_best_params, len(group1_symbols), len(group1_symbols))

# Train the model for Group 2
group2_model, group2_train_losses, group2_val_losses = train_model(group2_X_train_scaled, group2_Y_train, group2_X_val_scaled, group2_Y_val, group2_best_params, len(group2_symbols), len(group2_symbols))

# Plot training and validation loss for Group 1
plt.figure(figsize=(10, 5))
plt.plot(group1_train_losses, label='Train Loss - Group 1')
plt.plot(group1_val_losses, label='Val Loss - Group 1')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Training and Validation Loss - Group 1')
plt.show()

# Plot training and validation loss for Group 2
plt.figure(figsize=(10, 5))
plt.plot(group2_train_losses, label='Train Loss - Group 2')
plt.plot(group2_val_losses, label='Val Loss - Group 2')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Training and Validation Loss - Group 2')
plt.show()

# Function to make future predictions
def make_future_predictions(model, future_data, scaler, seq_length):
    model.eval()
    
    future_predictions = []
    current_seq = future_data[:seq_length]
    
    for i in range(len(future_data) - seq_length):
        current_seq_tensor = torch.tensor(current_seq).float().to(device).unsqueeze(0)
        with torch.no_grad():
            prediction = model(current_seq_tensor).cpu().numpy().flatten()
        future_predictions.append(prediction)
        
        current_seq = np.append(current_seq[1:], [future_data[seq_length + i]], axis=0)
    
    future_predictions = np.array(future_predictions)
    future_predictions = scaler.inverse_transform(future_predictions)
    return future_predictions

# Prepare future data for prediction
future_data_group1_aligned = group1.drop('Date', axis=1).values[-(seq_length + len(group1_X_test)):]
future_data_group2_aligned = group2.drop('Date', axis=1).values[-(seq_length + len(group2_X_test)):]

# Check if scaler is fitted properly
def is_fitted(scaler, n_features):
    try:
        # Test if it has the necessary attributes by transforming a sample with the correct number of features
        scaler.transform(np.zeros((1, n_features)))
        return True
    except (NotFittedError, ValueError):
        return False

# Correct the usage by passing the correct number of features
if is_fitted(scaler_group1, len(group1_symbols)) and is_fitted(scaler_group2, len(group2_symbols)):
    predictions_group1 = make_future_predictions(group1_model, future_data_group1_aligned, scaler_group1, seq_length)
    predictions_group2 = make_future_predictions(group2_model, future_data_group2_aligned, scaler_group2, seq_length)
else:
    predictions_group1 = None
    predictions_group2 = None
    print("Scalers are not fitted properly.")


# Plot the future predictions for each group
if predictions_group1 is not None:
    plt.figure(figsize=(12, 5))
    plt.plot(predictions_group1, label='Predicted Prices Group 1')
    plt.legend()
    plt.title('Future Predictions for Group 1')
    plt.show()

if predictions_group2 is not None:
    plt.figure(figsize=(12, 5))
    plt.plot(predictions_group2, label='Predicted Prices Group 2')
    plt.legend()
    plt.title('Future Predictions for Group 2')
    plt.show()

# Evaluate model performance
def evaluate_model_performance(y_true, y_pred):
    """
    Evaluate the performance of a regression model using MSE, MAE, and R² metrics.

    Parameters:
    - y_true: The ground truth target values.
    - y_pred: The predicted target values by the model.

    Returns:
    A dictionary containing MSE, MAE, and R².
    """
    mse = mean_squared_error(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return {'MSE': mse, 'MAE': mae, 'R²': r2}

# Assume we have ground truth for the test period for both groups
group1_Y_test_original = scaler_group1.inverse_transform(group1_Y_test)
group2_Y_test_original = scaler_group2.inverse_transform(group2_Y_test)

if predictions_group1 is not None:
    # Evaluate performance for Group 1
    performance_group1 = evaluate_model_performance(group1_Y_test_original, predictions_group1)
    print("Performance for Group 1:")
    print(f"MSE: {performance_group1['MSE']:.4f}")
    print(f"MAE: {performance_group1['MAE']:.4f}")
    print(f"R²: {performance_group1['R²']:.4f}")

if predictions_group2 is not None:
    # Evaluate performance for Group 2
    performance_group2 = evaluate_model_performance(group2_Y_test_original, predictions_group2)
    print("Performance for Group 2:")
    print(f"MSE: {performance_group2['MSE']:.4f}")
    print(f"MAE: {performance_group2['MAE']:.4f}")
    print(f"R²: {performance_group2['R²']:.4f}")

# Plotting the predictions versus actual for Group 1
if predictions_group1 is not None:
    plt.figure(figsize=(14, 7))
    plt.plot(group1_Y_test_original.flatten(), label='Actual Prices Group 1')
    plt.plot(predictions_group1.flatten(), label='Predicted Prices Group 1', linestyle='--')
    plt.title('Predicted vs Actual Prices for Group 1')
    plt.xlabel('Time Steps')
    plt.ylabel('Price')
    plt.legend()
    plt.show()

# Plotting the predictions versus actual for Group 2
if predictions_group2 is not None:
    plt.figure(figsize=(14, 7))
    plt.plot(group2_Y_test_original.flatten(), label='Actual Prices Group 2')
    plt.plot(predictions_group2.flatten(), label='Predicted Prices Group 2', linestyle='--')
    plt.title('Predicted vs Actual Prices for Group 2')
    plt.xlabel('Time Steps')
    plt.ylabel('Price')
    plt.legend()
    plt.show()

# Save the model weights (optional)
torch.save(group1_model.state_dict(), 'group1_model.pth')
torch.save(group2_model.state_dict(), 'group2_model.pth')
print("Models have been saved successfully.")
